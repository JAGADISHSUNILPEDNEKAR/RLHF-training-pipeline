# RLHF Training Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1FRDL6FjOqkULM8Rzs3_Niqkws_zeOvCt)

A modular, production-ready implementation of a **Reinforcement Learning from Human Feedback (RLHF)** pipeline. This project supports the full lifecycle of RLHF: **Annotation** (Human Feedback), **DPO** (Direct Preference Optimization), and **PPO** (Proximal Policy Optimization).

## ğŸ— Architecture

```mermaid
graph TD
    A[Base Model] -->|Load| B(Annotation UI)
    B -->|Human Preferences| C[Preference Dataset]
    C -->|Train| D[DPO Trainer]
    C -->|Train| E[Reward Model]
    E -->|Train| F[PPO Trainer]
    D --> G[Fine-tuned Model]
    F --> G
```

## ğŸš€ Features

- **Modular Design**: tailored for ease of use and extensibility.
- **Dual Training Modes**: Support for both DPO and PPO.
- **Interactive Annotation**: Built-in UI for collecting human feedback.
- **Configurable**: Centralized configuration for all hyperparameters.

## ğŸ“‚ Directory Structure

```
rlhf-pipeline/
â”œâ”€â”€ config/             # Configuration files
â”œâ”€â”€ data/               # Data storage
â”œâ”€â”€ notebooks/          # Jupyter notebooks for experimentation
â”œâ”€â”€ output/             # Model outputs
â”œâ”€â”€ scripts/            # CLI Execution scripts
â”œâ”€â”€ src/                # Source code
â”‚   â”œâ”€â”€ annotation.py   # Annotation UI logic
â”‚   â”œâ”€â”€ config.py       # Configuration settings
â”‚   â”œâ”€â”€ data.py         # Data processing
â”‚   â”œâ”€â”€ models.py       # Model loading
â”‚   â”œâ”€â”€ trainer_dpo.py  # DPO training
â”‚   â”œâ”€â”€ trainer_ppo.py  # PPO training
â”‚   â””â”€â”€ utils.py        # Utilities
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ pyproject.toml      # Project configuration & dependencies
â””â”€â”€ README.md           # Documentation
```

## ğŸ›  Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/YOUR_USERNAME/rlhf-pipeline.git
    cd rlhf-pipeline
    ```

2.  **Install dependencies:**
    ```bash
    pip install -e ".[dev]"
    ```
    This installs the package in editable mode along with development tools.

## ğŸƒ Usage

### 1. Annotation
Collect human preferences for your model responses.

**CLI Mode:**
```bash
python scripts/run_annotation.py
```

### 2. DPO Training
Train the model using Direct Preference Optimization.

```bash
python scripts/run_dpo.py
```
*Output will be saved to `output/dpo_model`.*

### 3. PPO Training
Fine-tune the model using Proximal Policy Optimization.

```bash
python scripts/run_ppo.py
```

## âš™ï¸ Configuration

Modify `src/config.py` to change parameters such as:
- `model_name` (default: "gpt2")
- `batch_size`
- `learning_rate`
- `output_dir`

## ğŸš€ Deployment

### Docker
Build and run the container:
```bash
docker build -t rlhf-pipeline .
docker run -p 7860:7860 rlhf-pipeline
```

### Hugging Face Spaces
This repository is configured for easy deployment on Hugging Face Spaces using the Docker SDK.
1. Create a new Space.
2. Select **Docker** as the SDK.
3. Push this code to the Space's repository.

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to get started.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
